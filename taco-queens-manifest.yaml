---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: helm-toolkit
data:
  chart_name: helm-toolkit
  release: helm-toolkit
  namespace: helm-tookit
  values: {}
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: helm-toolkit
    reference: 2.2.0
  dependencies: []
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ceph
data:
  chart_name: ceph
  release: ceph
  namespace: ceph
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-ceph
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - type: job 
          labels:
            application: ceph
            component: bootstrap
        - type: job 
          labels:
            application: ceph
            component: cephfs-client-key-generator
        - type: job 
          labels:
            application: ceph
            component: mds-keyring-generator
        - type: job 
          labels:
            application: ceph
            component: osd-keyring-generator
        - type: job 
          labels:
            application: ceph
            component: rgw-keyring-generator
        - type: job 
          labels:
            application: ceph
            component: mon-keyring-generator
        - type: job 
          labels:
            application: ceph
            component: mgr-keyring-generator
        - type: job 
          labels:
            application: ceph
            component: rbd-pool
        - type: job 
          labels:
            application: ceph
            component: storage-keys-generator
  values:
    endpoints:
      identity:
        namespace: openstack
      object_store:
        namespace: ceph
      ceph_mon:
        namespace: ceph
    network:
      public: 0.0.0.0/24
      cluster: 0.0.0.0/24
    deployment:
      storage_secrets: true
      ceph: true
      rbd_provisioner: true
      cephfs_provisioner: true
      client_secrets: false
      rgw_keystone_user_and_endpoints: false
    bootstrap:
      enabled: true
    conf:
      rgw_ks:
        enabled: true
      ceph:
        global:
          fsid: null
          osd_pool_default_size: 1
        osd:
          osd_crush_chooseleaf_type: 0
      pool:
        crush:
          tunables: null
        target:
          osd: 1
          pg_per_osd: 100
        default:
          crush_rule: same_host
        spec:
          # RBD pool
          - name: rbd
            application: rbd
            replication: 1
            percent_total_data: 40
          # CephFS pools
          - name: cephfs_metadata
            application: cephfs
            replication: 1
            percent_total_data: 5
          - name: cephfs_data
            application: cephfs
            replication: 1
            percent_total_data: 10
          # RadosGW pools
          - name: .rgw.root
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.control
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.data.root
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.gc
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.log
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.intent-log
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.meta
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.usage
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.keys
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.email
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.swift
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.users.uid
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.buckets.extra
            application: rgw
            replication: 1
            percent_total_data: 0.1
          - name: default.rgw.buckets.index
            application: rgw
            replication: 1
            percent_total_data: 3
          - name: default.rgw.buckets.data
            application: rgw
            replication: 1
            percent_total_data: 34.8
      storage:
        osd:
          - data:
              type: directory
              location: /var/lib/openstack-helm/ceph/osd/osd-one
            journal:
              type: directory
              location: /var/lib/openstack-helm/ceph/osd/journal-one
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: ceph
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ceph-openstack-config
data:
  chart_name: ceph-openstack-config
  release: ceph-openstack-config
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-ceph-openstack-config
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    endpoints:
      identity:
        namespace: "openstack"
      object_store:
        namespace: "ceph"
      ceph_mon:
        namespace: "ceph"
    ceph:
      enabled:
        mds: false
    deployment:
      storage_secrets: false
      ceph: false
      rbd_provisioner: false
      cephfs_provisioner: false
      client_secrets: true
      rgw_keystone_user_and_endpoints: false
    bootstrap:
      enabled: false
    storageclass:
      provision_storage_class: false
    manifests:
      daemonset_mon: false
      deployment_rgw: false
      deployment_moncheck: false
      deployment_rbd_provisioner: false
      deployment_cephfs_provisioner: false
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: ceph
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: mariadb
data:
  chart_name: mariadb
  release: mariadb
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-mariadb
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    pod:
      replicas:
        server: 1
    volume:
      enabled: false
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: mariadb
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: ingress
data:
  chart_name: ingress
  release: ingress
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-ingress
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    config:
      worker-processes: "8"
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: ingress
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: etcd
data:
  chart_name: etcd
  release: etcd
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-etcd
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    pod:
      replicas:
        etcd: 1
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: etcd
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: rabbitmq
data:
  chart_name: rabbitmq
  release: rabbitmq
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-rabbitmq
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    volume:
      enabled: false
    pod:
      replicas:
        server: 1
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: rabbitmq
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: memcached
data:
  chart_name: memcached
  release: memcached
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-memcached
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    pod:
      replicas:
        server: 1
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: memcached
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: libvirt
data:
  chart_name: libvirt
  release: libvirt
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-libvirt
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        libvirt: docker.io/kolla/ubuntu-source-nova-libvirt:queens
      pull_policy: Always
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: libvirt
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: openvswitch
data:
  chart_name: openvswitch
  release: openvswitch
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-openvswitch
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        openvswitch_db_server: docker.io/kolla/ubuntu-source-openvswitch-db-server:queens
        openvswitch_vswitchd: docker.io/kolla/ubuntu-source-openvswitch-vswitchd:queens
      pull_policy: Always
    network:
      external_bridge: br-ex
      interface:
        external: null
      auto_bridge_add: {}
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: openvswitch
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: keystone
data:
  chart_name: keystone
  release: keystone
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-keystone
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - name: keystone-bootstrap
          type: job
          labels:
            application: keystone
            component: bootstrap
        - name: keystone-credential-setup
          type: job
          labels:
            application: keystone
            component: credential-setup
        - name: keystone-db-init
          type: job
          labels:
            application: keystone
            component: db-init
        - name: keystone-db-sync
          type: job
          labels:
            application: keystone
            component: db-sync
        - name: keystone-fernet-setup
          type: job
          labels:
            application: keystone
            component: fernet-setup
  values:
    images:
      tags:
        bootstrap: docker.io/kolla/ubuntu-source-heat-engine:queens
        test: docker.io/kolla/ubuntu-source-rally:queens
        db_init: docker.io/kolla/ubuntu-source-heat-engine:queens
        keystone_db_sync: docker.io/kolla/ubuntu-source-keystone:queens
        db_drop: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_user: docker.io/kolla/ubuntu-source-heat-engine:queens
        keystone_fernet_setup: docker.io/kolla/ubuntu-source-keystone:queens
        keystone_fernet_rotate: docker.io/kolla/ubuntu-source-keystone:queens
        keystone_credential_setup: docker.io/kolla/ubuntu-source-keystone:queens
        keystone_credential_rotate: docker.io/kolla/ubuntu-source-keystone:queens
        keystone_api: docker.io/kolla/ubuntu-source-keystone:queens
        keystone_domain_manage: docker.io/kolla/ubuntu-source-keystone:queens
    conf:
      keystone:
        DEFAULT:
          debug: true
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: keystone
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: glance
data:
  chart_name: glance
  release: glance
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-glance
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - name: glance-bootstrap
          type: job
          labels:
            application: glance
            component: bootstrap
        - name: glance-storage-init
          type: job
          labels:
            application: glance
            component: storage-init
        - name: glance-db-init
          type: job
          labels:
            application: glance
            component: db-init
        - name: glance-db-sync
          type: job
          labels:
            application: glance
            component: db-sync
        - name: glance-ks-endpoints
          type: job
          labels:
            application: glance
            component: ks-endpoints
        - name: glance-ks-service
          type: job
          labels:
            application: glance
            component: ks-service
        - name: glance-ks-user
          type: job
          labels:
            application: glance
            component: ks-user
  values:
    storage: pvc
    images:
      tags:
        test: docker.io/kolla/ubuntu-source-rally:queens
        db_init: docker.io/kolla/ubuntu-source-heat-engine:queens
        glance_db_sync: docker.io/kolla/ubuntu-source-glance-api:queens
        db_drop: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_user: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_service: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_endpoints: docker.io/kolla/ubuntu-source-heat-engine:queens
        glance_api: docker.io/kolla/ubuntu-source-glance-api:queens
        glance_registry: docker.io/kolla/ubuntu-source-glance-registry:queens
        bootstrap: docker.io/kolla/ubuntu-source-heat-engine:queens
      pull_policy: Always
    pod:
      user:
        glance:
          uid: 42415
    network:
      api:
        ingress:
          proxy_body_size: 102400M
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: glance
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: nova
data:
  chart_name: nova
  release: nova
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-nova
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - name: nova-bootstrap
          type: job
          labels:
            application: nova
            component: bootstrap
        - name: nova-cell-setup
          type: job
          labels:
            application: nova
            component: cell-setup
        - name: nova-db-init
          type: job
          labels:
            application: nova
            component: db-init
        - name: nova-db-sync
          type: job
          labels:
            application: nova
            component: db-sync
        - name: nova-ks-endpoints
          type: job
          labels:
            application: nova
            component: ks-endpoints
        - name: nova-ks-service
          type: job
          labels:
            application: nova
            component: ks-service
        - name: nova-ks-user
          type: job
          labels:
            application: nova
            component: ks-user
        - name: placement-ks-endpoints
          type: job
          labels:
            application: placement
            component: ks-endpoints
        - name: placement-ks-service
          type: job
          labels:
            application: placement
            component: ks-service
        - name: placement-ks-user
          type: job
          labels:
            application: placement
            component: ks-user
  values:
    images:
      tags:
        test: docker.io/kolla/ubuntu-source-rally:queens
        db_drop: docker.io/kolla/ubuntu-source-heat-engine:queens
        db_init: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_user: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_service: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_endpoints: docker.io/kolla/ubuntu-source-heat-engine:queens
        nova_api: docker.io/kolla/ubuntu-source-nova-api:queens
        nova_cell_setup: docker.io/kolla/ubuntu-source-nova-api:queens
        nova_compute: docker.io/kolla/ubuntu-source-nova-compute:queens
        nova_compute_ironic: docker.io/kolla/ubuntu-source-nova-compute-ironic:queens
        nova_compute_ssh: docker.io/kolla/ubuntu-source-nova-ssh:queens
        nova_conductor: docker.io/kolla/ubuntu-source-nova-conductor:queens
        nova_consoleauth: docker.io/kolla/ubuntu-source-nova-consoleauth:queens
        nova_db_sync: docker.io/kolla/ubuntu-source-nova-api:queens
        nova_novncproxy: docker.io/kolla/ubuntu-source-nova-novncproxy:queens
        nova_novncproxy_assets: docker.io/kolla/ubuntu-source-nova-novncproxy:queens
        nova_placement: docker.io/kolla/ubuntu-source-nova-placement-api:queens
        nova_scheduler: docker.io/kolla/ubuntu-source-nova-scheduler:queens
        nova_spiceproxy: docker.io/kolla/ubuntu-source-nova-spicehtml5proxy:queens
        nova_spiceproxy_assets: docker.io/kolla/ubuntu-source-nova-spicehtml5proxy:queens
        bootstrap: docker.io/kolla/ubuntu-source-heat-engine:queens
      pull_policy: Always
    network:
      novncproxy:
        name: "nova-novncproxy"
        node_port:
          enabled: true
          port: 30608
        port: 6080
        targetPort: 6080
    conf:
      rootwrap_filters:
        api_metadata:
          override: |
            # nova-rootwrap command filters for api-metadata nodes
            # This is needed on nova-api hosts running with "metadata" in enabled_apis
            # or when running nova-api-metadata
            # This file should be owned by (and only-writeable by) the root user
            [Filters]
            # nova/network/linux_net.py: 'ip[6]tables-save' % (cmd, '-t', ...
            iptables-save: CommandFilter, iptables-save, root
            ip6tables-save: CommandFilter, ip6tables-save, root
            # nova/network/linux_net.py: 'ip[6]tables-restore' % (cmd,)
            iptables-restore: CommandFilter, iptables-restore, root
            ip6tables-restore: CommandFilter, ip6tables-restore, root
          append:
        compute:
          override: |
            # nova-rootwrap command filters for compute nodes
            # This file should be owned by (and only-writeable by) the root user
            [Filters]
            # nova/virt/xenapi/vm_utils.py: tune2fs, -O ^has_journal, part_path
            # nova/virt/xenapi/vm_utils.py: tune2fs, -j, partition_path
            tune2fs: CommandFilter, tune2fs, root
            # nova/virt/libvirt/utils.py: 'blockdev', '--getsize64', path
            # nova/virt/disk/mount/nbd.py: 'blockdev', '--flushbufs', device
            blockdev: RegExpFilter, blockdev, root, blockdev, (--getsize64|--flushbufs), /dev/.*
            # nova/virt/libvirt/vif.py: 'ip', 'tuntap', 'add', dev, 'mode', 'tap'
            # nova/virt/libvirt/vif.py: 'ip', 'link', 'set', dev, 'up'
            # nova/virt/libvirt/vif.py: 'ip', 'link', 'delete', dev
            # nova/network/linux_net.py: 'ip', 'addr', 'add', str(floating_ip)+'/32'i..
            # nova/network/linux_net.py: 'ip', 'addr', 'del', str(floating_ip)+'/32'..
            # nova/network/linux_net.py: 'ip', 'addr', 'add', '169.254.169.254/32',..
            # nova/network/linux_net.py: 'ip', 'addr', 'show', 'dev', dev, 'scope',..
            # nova/network/linux_net.py: 'ip', 'addr', 'del/add', ip_params, dev)
            # nova/network/linux_net.py: 'ip', 'addr', 'del', params, fields[-1]
            # nova/network/linux_net.py: 'ip', 'addr', 'add', params, bridge
            # nova/network/linux_net.py: 'ip', '-f', 'inet6', 'addr', 'change', ..
            # nova/network/linux_net.py: 'ip', 'link', 'set', 'dev', dev, 'promisc',..
            # nova/network/linux_net.py: 'ip', 'link', 'add', 'link', bridge_if ...
            # nova/network/linux_net.py: 'ip', 'link', 'set', interface, address,..
            # nova/network/linux_net.py: 'ip', 'link', 'set', interface, 'up'
            # nova/network/linux_net.py: 'ip', 'link', 'set', bridge, 'up'
            # nova/network/linux_net.py: 'ip', 'addr', 'show', 'dev', interface, ..
            # nova/network/linux_net.py: 'ip', 'link', 'set', dev, address, ..
            # nova/network/linux_net.py: 'ip', 'link', 'set', dev, 'up'
            # nova/network/linux_net.py: 'ip', 'route', 'add', ..
            # nova/network/linux_net.py: 'ip', 'route', 'del', .
            # nova/network/linux_net.py: 'ip', 'route', 'show', 'dev', dev
            ip: CommandFilter, ip, root
            # nova/virt/libvirt/vif.py: 'tunctl', '-b', '-t', dev
            # nova/network/linux_net.py: 'tunctl', '-b', '-t', dev
            tunctl: CommandFilter, tunctl, root
            # nova/virt/libvirt/vif.py: 'ovs-vsctl', ...
            # nova/virt/libvirt/vif.py: 'ovs-vsctl', 'del-port', ...
            # nova/network/linux_net.py: 'ovs-vsctl', ....
            ovs-vsctl: CommandFilter, ovs-vsctl, root
            # nova/network/linux_net.py: 'ivs-ctl', ....
            ivs-ctl: CommandFilter, ivs-ctl, root
            # nova/network/linux_net.py: 'ovs-ofctl', ....
            ovs-ofctl: CommandFilter, ovs-ofctl, root
            # nova/virt/xenapi/volume_utils.py: 'iscsiadm', '-m', ...
            iscsiadm: CommandFilter, iscsiadm, root
            # nova/virt/libvirt/volume/aoe.py: 'aoe-revalidate', aoedev
            # nova/virt/libvirt/volume/aoe.py: 'aoe-discover'
            aoe-revalidate: CommandFilter, aoe-revalidate, root
            aoe-discover: CommandFilter, aoe-discover, root
            # nova/virt/xenapi/vm_utils.py: 'pygrub', '-qn', dev_path
            pygrub: CommandFilter, pygrub, root
            # nova/virt/xenapi/vm_utils.py: fdisk %(dev_path)s
            fdisk: CommandFilter, fdisk, root
            # nova/virt/xenapi/vm_utils.py: e2fsck, -f, -p, partition_path
            # nova/virt/disk/api.py: e2fsck, -f, -p, image
            e2fsck: CommandFilter, e2fsck, root
            # nova/virt/xenapi/vm_utils.py: resize2fs, partition_path
            # nova/virt/disk/api.py: resize2fs, image
            resize2fs: CommandFilter, resize2fs, root
            # nova/network/linux_net.py: 'ip[6]tables-save' % (cmd, '-t', ...
            iptables-save: CommandFilter, iptables-save, root
            ip6tables-save: CommandFilter, ip6tables-save, root
            # nova/network/linux_net.py: 'ip[6]tables-restore' % (cmd,)
            iptables-restore: CommandFilter, iptables-restore, root
            ip6tables-restore: CommandFilter, ip6tables-restore, root
            # nova/network/linux_net.py: 'arping', '-U', floating_ip, '-A', '-I', ...
            # nova/network/linux_net.py: 'arping', '-U', network_ref['dhcp_server'],..
            arping: CommandFilter, arping, root
            # nova/network/linux_net.py: 'dhcp_release', dev, address, mac_address
            dhcp_release: CommandFilter, dhcp_release, root
            # nova/network/linux_net.py: 'kill', '-9', pid
            # nova/network/linux_net.py: 'kill', '-HUP', pid
            kill_dnsmasq: KillFilter, root, /usr/sbin/dnsmasq, -9, -HUP
            # nova/network/linux_net.py: 'kill', pid
            kill_radvd: KillFilter, root, /usr/sbin/radvd
            # nova/network/linux_net.py: dnsmasq call
            dnsmasq: EnvFilter, env, root, CONFIG_FILE=, NETWORK_ID=, dnsmasq
            # nova/network/linux_net.py: 'radvd', '-C', '%s' % _ra_file(dev, 'conf'..
            radvd: CommandFilter, radvd, root
            # nova/network/linux_net.py: 'brctl', 'addbr', bridge
            # nova/network/linux_net.py: 'brctl', 'setfd', bridge, 0
            # nova/network/linux_net.py: 'brctl', 'stp', bridge, 'off'
            # nova/network/linux_net.py: 'brctl', 'addif', bridge, interface
            brctl: CommandFilter, brctl, root
            # nova/virt/libvirt/utils.py: 'mkswap'
            # nova/virt/xenapi/vm_utils.py: 'mkswap'
            mkswap: CommandFilter, mkswap, root
            # nova/virt/xenapi/vm_utils.py: 'mkfs'
            # nova/utils.py: 'mkfs', fs, path, label
            mkfs: CommandFilter, mkfs, root
            # nova/virt/libvirt/utils.py: 'qemu-img'
            qemu-img: CommandFilter, qemu-img, root
            # nova/virt/disk/api.py:
            mkfs.ext3: CommandFilter, mkfs.ext3, root
            mkfs.ext4: CommandFilter, mkfs.ext4, root
            mkfs.ntfs: CommandFilter, mkfs.ntfs, root
            # os-brick needed commands
            read_initiator: ReadFileFilter, /etc/iscsi/initiatorname.iscsi
            multipath: CommandFilter, multipath, root
            # multipathd show status
            multipathd: CommandFilter, multipathd, root
            systool: CommandFilter, systool, root
            vgc-cluster: CommandFilter, vgc-cluster, root
            # os_brick/initiator/connector.py
            drv_cfg: CommandFilter, /opt/emc/scaleio/sdc/bin/drv_cfg, root, /opt/emc/scaleio/sdc/bin/drv_cfg, --query_guid
            # TODO(smcginnis) Temporary fix.
            # Need to pull in os-brick os-brick.filters file instead and clean
            # out stale brick values from this file.
            scsi_id: CommandFilter, /lib/udev/scsi_id, root
            # os_brick.privileged.default oslo.privsep context
            # This line ties the superuser privs with the config files, context name,
            # and (implicitly) the actual python code invoked.
            privsep-rootwrap-os_brick: RegExpFilter, privsep-helper, root, privsep-helper, --config-file, /etc/(?!\.\.).*, --privsep_context, os_brick.privileged.default, --privsep_sock_path, /tmp/.*
            privsep-rootwrap-sys_admin: RegExpFilter, privsep-helper, root, privsep-helper, --config-file, /etc/(?!\.\.).*, --privsep_context, nova.privsep.sys_admin_pctxt, --privsep_sock_path, /tmp/.*
            # nova/virt/libvirt/storage/dmcrypt.py:
            cryptsetup: CommandFilter, cryptsetup, root
            # nova/virt/xenapi/vm_utils.py:
            xenstore-read: CommandFilter, xenstore-read, root
            # nova/virt/libvirt/utils.py:
            rbd: CommandFilter, rbd, root
            # nova/virt/libvirt/volume/volume.py: 'cp', '/dev/stdin', delete_control..
            cp: CommandFilter, cp, root
            # nova/virt/xenapi/vm_utils.py:
            sync: CommandFilter, sync, root
            # nova/virt/libvirt/volume/vzstorage.py
            pstorage-mount: CommandFilter, pstorage-mount, root
          append:
        network:
          override: |
            # nova-rootwrap command filters for network nodes
            # This file should be owned by (and only-writeable by) the root user
            [Filters]
            # nova/virt/libvirt/vif.py: 'ip', 'tuntap', 'add', dev, 'mode', 'tap'
            # nova/virt/libvirt/vif.py: 'ip', 'link', 'set', dev, 'up'
            # nova/virt/libvirt/vif.py: 'ip', 'link', 'delete', dev
            # nova/network/linux_net.py: 'ip', 'addr', 'add', str(floating_ip)+'/32'i..
            # nova/network/linux_net.py: 'ip', 'addr', 'del', str(floating_ip)+'/32'..
            # nova/network/linux_net.py: 'ip', 'addr', 'add', '169.254.169.254/32',..
            # nova/network/linux_net.py: 'ip', 'addr', 'show', 'dev', dev, 'scope',..
            # nova/network/linux_net.py: 'ip', 'addr', 'del/add', ip_params, dev)
            # nova/network/linux_net.py: 'ip', 'addr', 'del', params, fields[-1]
            # nova/network/linux_net.py: 'ip', 'addr', 'add', params, bridge
            # nova/network/linux_net.py: 'ip', '-f', 'inet6', 'addr', 'change', ..
            # nova/network/linux_net.py: 'ip', 'link', 'set', 'dev', dev, 'promisc',..
            # nova/network/linux_net.py: 'ip', 'link', 'add', 'link', bridge_if ...
            # nova/network/linux_net.py: 'ip', 'link', 'set', interface, address,..
            # nova/network/linux_net.py: 'ip', 'link', 'set', interface, 'up'
            # nova/network/linux_net.py: 'ip', 'link', 'set', bridge, 'up'
            # nova/network/linux_net.py: 'ip', 'addr', 'show', 'dev', interface, ..
            # nova/network/linux_net.py: 'ip', 'link', 'set', dev, address, ..
            # nova/network/linux_net.py: 'ip', 'link', 'set', dev, 'up'
            # nova/network/linux_net.py: 'ip', 'route', 'add', ..
            # nova/network/linux_net.py: 'ip', 'route', 'del', .
            # nova/network/linux_net.py: 'ip', 'route', 'show', 'dev', dev
            ip: CommandFilter, ip, root
            # nova/virt/libvirt/vif.py: 'ovs-vsctl', ...
            # nova/virt/libvirt/vif.py: 'ovs-vsctl', 'del-port', ...
            # nova/network/linux_net.py: 'ovs-vsctl', ....
            ovs-vsctl: CommandFilter, ovs-vsctl, root
            # nova/network/linux_net.py: 'ovs-ofctl', ....
            ovs-ofctl: CommandFilter, ovs-ofctl, root
            # nova/virt/libvirt/vif.py: 'ivs-ctl', ...
            # nova/virt/libvirt/vif.py: 'ivs-ctl', 'del-port', ...
            # nova/network/linux_net.py: 'ivs-ctl', ....
            ivs-ctl: CommandFilter, ivs-ctl, root
            # nova/virt/libvirt/vif.py: 'ifc_ctl', ...
            ifc_ctl: CommandFilter, /opt/pg/bin/ifc_ctl, root
            # nova/network/linux_net.py: 'ebtables', '-D' ...
            # nova/network/linux_net.py: 'ebtables', '-I' ...
            ebtables: CommandFilter, ebtables, root
            ebtables_usr: CommandFilter, ebtables, root
            # nova/network/linux_net.py: 'ip[6]tables-save' % (cmd, '-t', ...
            iptables-save: CommandFilter, iptables-save, root
            ip6tables-save: CommandFilter, ip6tables-save, root
            # nova/network/linux_net.py: 'ip[6]tables-restore' % (cmd,)
            iptables-restore: CommandFilter, iptables-restore, root
            ip6tables-restore: CommandFilter, ip6tables-restore, root
            # nova/network/linux_net.py: 'arping', '-U', floating_ip, '-A', '-I', ...
            # nova/network/linux_net.py: 'arping', '-U', network_ref['dhcp_server'],..
            arping: CommandFilter, arping, root
            # nova/network/linux_net.py: 'dhcp_release', dev, address, mac_address
            dhcp_release: CommandFilter, dhcp_release, root
            # nova/network/linux_net.py: 'kill', '-9', pid
            # nova/network/linux_net.py: 'kill', '-HUP', pid
            kill_dnsmasq: KillFilter, root, /usr/sbin/dnsmasq, -9, -HUP
            # nova/network/linux_net.py: 'kill', pid
            kill_radvd: KillFilter, root, /usr/sbin/radvd
            # nova/network/linux_net.py: dnsmasq call
            dnsmasq: EnvFilter, env, root, CONFIG_FILE=, NETWORK_ID=, dnsmasq
            # nova/network/linux_net.py: 'radvd', '-C', '%s' % _ra_file(dev, 'conf'..
            radvd: CommandFilter, radvd, root
            # nova/network/linux_net.py: 'brctl', 'addbr', bridge
            # nova/network/linux_net.py: 'brctl', 'setfd', bridge, 0
            # nova/network/linux_net.py: 'brctl', 'stp', bridge, 'off'
            # nova/network/linux_net.py: 'brctl', 'addif', bridge, interface
            brctl: CommandFilter, brctl, root
            # nova/network/linux_net.py: 'sysctl', ....
            sysctl: CommandFilter, sysctl, root
            # nova/network/linux_net.py: 'conntrack'
            conntrack: CommandFilter, conntrack, root
            # nova/network/linux_net.py: 'fp-vdev'
            fp-vdev: CommandFilter, fp-vdev, root
          append:
      nova:
        DEFAULT:
          scheduler_default_filters: "RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter"
          debug: true
        vnc:
          vncserver_proxyclient_address: taco-aio
          novncproxy_base_url: http://taco-aio:30608/vnc_auto.html
        scheduler:
          discover_hosts_in_cells_interval: 60
        libvirt:
          virt_type: "qemu"
    endpoints:
      oslo_db_cell0:
        path: /nova_cell0
    pod:
      user:
        nova:
          uid: 42436
      replicas:
        api_metadata: 1
        osapi: 1
        conductor: 1
        consoleauth: 1
        scheduler: 1
        novncproxy: 1
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: nova
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: neutron
data:
  chart_name: neutron
  release: neutron
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-neutron
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - name: neutron-db-init
          type: job
          labels:
            application: neutron
            component: db-init
        - name: neutron-db-sync
          type: job
          labels:
            application: neutron
            component: db-sync
        - name: neutron-ks-endpoints
          type: job
          labels:
            application: neutron
            component: ks-endpoints
        - name: neutron-ks-service
          type: job
          labels:
            application: neutron
            component: ks-service
        - name: neutron-ks-user
          type: job
          labels:
            application: neutron
            component: ks-user
  values:
    images:
      tags:
        bootstrap: docker.io/kolla/ubuntu-source-heat-engine:queens
        db_init: docker.io/kolla/ubuntu-source-heat-engine:queens
        neutron_db_sync: docker.io/kolla/ubuntu-source-neutron-server:queens
        db_drop: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_user: docker.io/kolla/ubuntu-source-kolla-toolbox:queens
        ks_service: docker.io/kolla/ubuntu-source-kolla-toolbox:queens
        ks_endpoints: docker.io/kolla/ubuntu-source-kolla-toolbox:queens
        neutron_server: docker.io/kolla/ubuntu-source-neutron-server:queens
        neutron_dhcp: docker.io/kolla/ubuntu-source-neutron-dhcp-agent:queens
        neutron_metadata: docker.io/kolla/ubuntu-source-neutron-metadata-agent:queens
        neutron_l3: docker.io/kolla/ubuntu-source-neutron-l3-agent:queens
        neutron_openvswitch_agent: docker.io/kolla/ubuntu-source-neutron-openvswitch-agent:queens
        neutron_linuxbridge_agent: docker.io/kolla/ubuntu-source-neutron-linuxbridge-agent:queens
      pull_policy: Always
    labels:
      agent:
        metadata:
          # use config drive for medata service
          node_selector_value: "disabled"
    pod:
      user:
        neutron:
          uid: 42435
    conf:
      rootwrap_filters:
        debug:
          override: |
            # neutron-rootwrap command filters for nodes on which neutron is
            # expected to control network
            #
            # This file should be owned by (and only-writeable by) the root user
            # format seems to be
            # cmd-name: filter-name, raw-command, user, args
            [Filters]
            # This is needed because we should ping
            # from inside a namespace which requires root
            # _alt variants allow to match -c and -w in any order
            #   (used by NeutronDebugAgent.ping_all)
            ping: RegExpFilter, ping, root, ping, -w, \d+, -c, \d+, [0-9\.]+
            ping_alt: RegExpFilter, ping, root, ping, -c, \d+, -w, \d+, [0-9\.]+
            ping6: RegExpFilter, ping6, root, ping6, -w, \d+, -c, \d+, [0-9A-Fa-f:]+
            ping6_alt: RegExpFilter, ping6, root, ping6, -c, \d+, -w, \d+, [0-9A-Fa-f:]+
          append:
        dhcp:
          override: |
            # neutron-rootwrap command filters for nodes on which neutron is
            # expected to control network
            #
            # This file should be owned by (and only-writeable by) the root user
            # format seems to be
            # cmd-name: filter-name, raw-command, user, args
            [Filters]
            # dhcp-agent
            dnsmasq: CommandFilter, dnsmasq, root
            # dhcp-agent uses kill as well, that's handled by the generic KillFilter
            # it looks like these are the only signals needed, per
            # neutron/agent/linux/dhcp.py
            kill_dnsmasq: KillFilter, root, /sbin/dnsmasq, -9, -HUP, -15
            kill_dnsmasq_usr: KillFilter, root, /usr/sbin/dnsmasq, -9, -HUP, -15
            ovs-vsctl: CommandFilter, ovs-vsctl, root
            ivs-ctl: CommandFilter, ivs-ctl, root
            mm-ctl: CommandFilter, mm-ctl, root
            dhcp_release: CommandFilter, dhcp_release, root
            dhcp_release6: CommandFilter, dhcp_release6, root
            # haproxy
            haproxy: RegExpFilter, haproxy, root, haproxy, -f, .*
            kill_haproxy: KillFilter, root, haproxy, -15, -9, -HUP
            # RHEL invocation of the metadata proxy will report /usr/bin/python
            # TODO(dalvarez): Remove kill_metadata* filters in Q release since
            # neutron-ns-metadata-proxy is now replaced by haproxy. We keep them for now
            # for the migration process
            kill_metadata: KillFilter, root, python, -9
            kill_metadata7: KillFilter, root, python2.7, -9
            kill_metadata35: KillFilter, root, python3.5, -9
            # ip_lib
            ip: IpFilter, ip, root
            find: RegExpFilter, find, root, find, /sys/class/net, -maxdepth, 1, -type, l, -printf, %.*
            ip_exec: IpNetnsExecFilter, ip, root
          append:
        dibbler:
          override: |
            # format seems to be
            # cmd-name: filter-name, raw-command, user, args
            [Filters]
            # Filters for the dibbler-based reference implementation of the pluggable
            # Prefix Delegation driver. Other implementations using an alternative agent
            # should include a similar filter in this folder.
            # prefix_delegation_agent
            dibbler-client: CommandFilter, dibbler-client, root
            kill_dibbler-client: KillFilter, root, dibbler-client, -9
          append:
        ebtables:
          override: |
            # neutron-rootwrap command filters for nodes on which neutron is
            # expected to control network
            #
            # This file should be owned by (and only-writeable by) the root user
            # format seems to be
            # cmd-name: filter-name, raw-command, user, args
            [Filters]
            ebtables: CommandFilter, ebtables, root
          append:
        ipset_firewall:
          override: |
            # neutron-rootwrap command filters for nodes on which neutron is
            # expected to control network
            #
            # This file should be owned by (and only-writeable by) the root user
            # format seems to be
            # cmd-name: filter-name, raw-command, user, args
            [Filters]
            # neutron/agent/linux/iptables_firewall.py
            #   "ipset", "-A", ...
            ipset: CommandFilter, ipset, root
          append:
        iptables_firewall:
          override: |
            # neutron-rootwrap command filters for nodes on which neutron is
            # expected to control network
            #
            # This file should be owned by (and only-writeable by) the root user
            # format seems to be
            # cmd-name: filter-name, raw-command, user, args
            [Filters]
            # neutron/agent/linux/iptables_firewall.py
            #   "iptables-save", ...
            iptables-save: CommandFilter, iptables-save, root
            iptables-restore: CommandFilter, iptables-restore, root
            ip6tables-save: CommandFilter, ip6tables-save, root
            ip6tables-restore: CommandFilter, ip6tables-restore, root
            # neutron/agent/linux/iptables_firewall.py
            #   "iptables", "-A", ...
            iptables: CommandFilter, iptables, root
            ip6tables: CommandFilter, ip6tables, root
            # neutron/agent/linux/ip_conntrack.py
            conntrack: CommandFilter, conntrack, root
          append:
        l3:
          override: |
            # neutron-rootwrap command filters for nodes on which neutron is
            # expected to control network
            #
            # This file should be owned by (and only-writeable by) the root user
            # format seems to be
            # cmd-name: filter-name, raw-command, user, args
            [Filters]
            # arping
            arping: CommandFilter, arping, root
            # l3_agent
            sysctl: CommandFilter, sysctl, root
            route: CommandFilter, route, root
            radvd: CommandFilter, radvd, root
            # haproxy
            haproxy: RegExpFilter, haproxy, root, haproxy, -f, .*
            kill_haproxy: KillFilter, root, haproxy, -15, -9, -HUP
            # RHEL invocation of the metadata proxy will report /usr/bin/python
            # TODO(dalvarez): Remove kill_metadata* filters in Q release since
            # neutron-ns-metadata-proxy is now replaced by haproxy. We keep them for now
            # for the migration process
            kill_metadata: KillFilter, root, python, -15, -9
            kill_metadata7: KillFilter, root, python2.7, -15, -9
            kill_metadata35: KillFilter, root, python3.5, -15, -9
            kill_radvd_usr: KillFilter, root, /usr/sbin/radvd, -15, -9, -HUP
            kill_radvd: KillFilter, root, /sbin/radvd, -15, -9, -HUP
            # ip_lib
            ip: IpFilter, ip, root
            find: RegExpFilter, find, root, find, /sys/class/net, -maxdepth, 1, -type, l, -printf, %.*
            ip_exec: IpNetnsExecFilter, ip, root
            # l3_tc_lib
            l3_tc_show_qdisc: RegExpFilter, tc, root, tc, qdisc, show, dev, .+
            l3_tc_add_qdisc_ingress: RegExpFilter, tc, root, tc, qdisc, add, dev, .+, ingress
            l3_tc_add_qdisc_egress: RegExpFilter, tc, root, tc, qdisc, add, dev, .+, root, handle, 1:, htb
            l3_tc_show_filters: RegExpFilter, tc, root, tc, -p, -s, -d, filter, show, dev, .+, parent, .+, prio, 1
            l3_tc_delete_filters: RegExpFilter, tc, root, tc, filter, del, dev, .+, parent, .+, prio, 1, handle, .+, u32
            l3_tc_add_filter_ingress: RegExpFilter, tc, root, tc, filter, add, dev, .+, parent, .+, protocol, ip, prio, 1, u32, match, ip, dst, .+, police, rate, .+, burst, .+, drop, flowid, :1
            l3_tc_add_filter_egress:  RegExpFilter, tc, root, tc, filter, add, dev, .+, parent, .+, protocol, ip, prio, 1, u32, match, ip, src, .+, police, rate, .+, burst, .+, drop, flowid, :1
            # For ip monitor
            kill_ip_monitor: KillFilter, root, ip, -9
            # ovs_lib (if OVSInterfaceDriver is used)
            ovs-vsctl: CommandFilter, ovs-vsctl, root
            # iptables_manager
            iptables-save: CommandFilter, iptables-save, root
            iptables-restore: CommandFilter, iptables-restore, root
            ip6tables-save: CommandFilter, ip6tables-save, root
            ip6tables-restore: CommandFilter, ip6tables-restore, root
            # Keepalived
            keepalived: CommandFilter, keepalived, root
            kill_keepalived: KillFilter, root, /usr/sbin/keepalived, -HUP, -15, -9
            # l3 agent to delete floatingip's conntrack state
            conntrack: CommandFilter, conntrack, root
            # keepalived state change monitor
            keepalived_state_change: CommandFilter, neutron-keepalived-state-change, root
          append:
        linuxbridge_plugin:
          override: |
            # neutron-rootwrap command filters for nodes on which neutron is
            # expected to control network
            #
            # This file should be owned by (and only-writeable by) the root user
            # format seems to be
            # cmd-name: filter-name, raw-command, user, args
            [Filters]
            # linuxbridge-agent
            # unclear whether both variants are necessary, but I'm transliterating
            # from the old mechanism
            brctl: CommandFilter, brctl, root
            bridge: CommandFilter, bridge, root
            sysctl: CommandFilter, sysctl, root
            # ip_lib
            ip: IpFilter, ip, root
            find: RegExpFilter, find, root, find, /sys/class/net, -maxdepth, 1, -type, l, -printf, %.*
            ip_exec: IpNetnsExecFilter, ip, root
            # tc commands needed for QoS support
            tc_replace_tbf: RegExpFilter, tc, root, tc, qdisc, replace, dev, .+, root, tbf, rate, .+, latency, .+, burst, .+
            tc_add_ingress: RegExpFilter, tc, root, tc, qdisc, add, dev, .+, ingress, handle, .+
            tc_delete: RegExpFilter, tc, root, tc, qdisc, del, dev, .+, .+
            tc_show_qdisc: RegExpFilter, tc, root, tc, qdisc, show, dev, .+
            tc_show_filters: RegExpFilter, tc, root, tc, filter, show, dev, .+, parent, .+
            tc_add_filter: RegExpFilter, tc, root, tc, filter, add, dev, .+, parent, .+, protocol, all, prio, .+, basic, police, rate, .+, burst, .+, mtu, .+, drop
          append:
        netns_cleanup:
          override: |
            # neutron-rootwrap command filters for nodes on which neutron is
            # expected to control network
            #
            # This file should be owned by (and only-writeable by) the root user
            # format seems to be
            # cmd-name: filter-name, raw-command, user, args
            [Filters]
            # netns-cleanup
            netstat: CommandFilter, netstat, root
          append:
        openvswitch_plugin:
          override: |
            # neutron-rootwrap command filters for nodes on which neutron is
            # expected to control network
            #
            # This file should be owned by (and only-writeable by) the root user
            # format seems to be
            # cmd-name: filter-name, raw-command, user, args
            [Filters]
            # openvswitch-agent
            # unclear whether both variants are necessary, but I'm transliterating
            # from the old mechanism
            ovs-vsctl: CommandFilter, ovs-vsctl, root
            # NOTE(yamamoto): of_interface=native doesn't use ovs-ofctl
            ovs-ofctl: CommandFilter, ovs-ofctl, root
            kill_ovsdb_client: KillFilter, root, /usr/bin/ovsdb-client, -9
            ovsdb-client: CommandFilter, ovsdb-client, root
            # ip_lib
            ip: IpFilter, ip, root
            find: RegExpFilter, find, root, find, /sys/class/net, -maxdepth, 1, -type, l, -printf, %.*
            ip_exec: IpNetnsExecFilter, ip, root
            # needed for FDB extension
            bridge: CommandFilter, bridge, root
          append:
        privsep:
          override: |
            # Command filters to allow privsep daemon to be started via rootwrap.
            #
            # This file should be owned by (and only-writeable by) the root user
            [Filters]
            # By installing the following, the local admin is asserting that:
            #
            # 1. The python module load path used by privsep-helper
            #    command as root (as started by sudo/rootwrap) is trusted.
            # 2. Any oslo.config files matching the --config-file
            #    arguments below are trusted.
            # 3. Users allowed to run sudo/rootwrap with this configuration(*) are
            #    also allowed to invoke python "entrypoint" functions from
            #    --privsep_context with the additional (possibly root) privileges
            #    configured for that context.
            #
            # (*) ie: the user is allowed by /etc/sudoers to run rootwrap as root
            #
            # In particular, the oslo.config and python module path must not
            # be writeable by the unprivileged user.
            # oslo.privsep default neutron context
            privsep: PathFilter, privsep-helper, root,
             --config-file, /etc,
             --privsep_context, neutron.privileged.default,
             --privsep_sock_path, /
            # NOTE: A second `--config-file` arg can also be added above. Since
            # many neutron components are installed like that (eg: by devstack).
            # Adjust to suit local requirements.
          append:
      neutron_sudoers:
        override: |
          # This sudoers file supports rootwrap and rootwrap-daemon for both Kolla and LOCI Images.
          Defaults !requiretty
          Defaults secure_path="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/var/lib/openstack/bin:/var/lib/kolla/venv/bin"
          neutron ALL = (root) NOPASSWD: /var/lib/kolla/venv/bin/neutron-rootwrap /etc/neutron/rootwrap.conf *, /var/lib/openstack/bin/neutron-rootwrap /etc/neutron/rootwrap.conf *, /var/lib/kolla/venv/bin/neutron-rootwrap-daemon /etc/neutron/rootwrap.conf, /var/lib/openstack/bin/neutron-rootwrap-daemon /etc/neutron/rootwrap.conf
      neutron:
        DEFAULT:
          core_plugin: ml2
          service_plugins: router
          l3_ha: False
          interface_driver: openvswitch
        agent:
          root_helper: sudo neutron-rootwrap /etc/neutron/rootwrap.conf
          root_helper_daemon: sudo neutron-rootwrap-daemon /etc/neutron/rootwrap.conf
      plugins:
        ml2_conf:
          ml2:
            mechanism_drivers: openvswitch,l2population
            type_drivers: flat, vxlan
            tenant_network_types: vxlan
        openvswitch_agent:
          agent:
            tunnel_types: vxlan
          ovs:
            bridge_mappings: external:br-ex
          securitygroup:
            # use iptables_hybrid for kernel versions lower than 4.3
            firewall_driver: openvswitch
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: neutron
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: cinder
data:
  chart_name: cinder
  release: cinder
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-cinder
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
    pre:
      delete:
        - name: cinder-bootstrap
          type: job
          labels:
            application: cinder
            component: bootstrap
        - name: cinder-db-init
          type: job
          labels:
            application: cinder
            component: db-init
        - name: cinder-db-sync
          type: job
          labels:
            application: cinder
            component: db-sync
        - name: cinder-ks-endpoints
          type: job
          labels:
            application: cinder
            component: ks-endpoints
        - name: cinder-ks-service
          type: job
          labels:
            application: cinder
            component: ks-service
        - name: cinder-ks-user
          type: job
          labels:
            application: cinder
            component: ks-user
  values:
    conf:
      cinder:
        DEFAULT:
          debug: true
    images:
      tags:
        db_init: docker.io/kolla/ubuntu-source-heat-engine:queens
        cinder_db_sync: docker.io/kolla/ubuntu-source-cinder-api:queens
        db_drop: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_user: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_service: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_endpoints: docker.io/kolla/ubuntu-source-heat-engine:queens
        cinder_api: docker.io/kolla/ubuntu-source-cinder-api:queens
        bootstrap: docker.io/kolla/ubuntu-source-heat-engine:queens
        cinder_scheduler: docker.io/kolla/ubuntu-source-cinder-scheduler:queens
        cinder_volume: docker.io/kolla/ubuntu-source-cinder-volume:queens
        cinder_volume_usage_audit: docker.io/kolla/ubuntu-source-cinder-volume:queens
        cinder_backup: docker.io/kolla/ubuntu-source-cinder-backup:queens
      pull_policy: Always
    pod:
      user:
        cinder:
          uid: 42407
      replicas:
        api: 1
        backup: 1
        scheduler: 1
        volume: 1
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: cinder
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: heat
data:
  chart_name: heat
  release: heat
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-heat
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        bootstrap: docker.io/kolla/ubuntu-source-heat-engine:queens
        db_init: docker.io/kolla/ubuntu-source-heat-engine:queens
        heat_db_sync: docker.io/kolla/ubuntu-source-heat-api:queens
        db_drop: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_user: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_service: docker.io/kolla/ubuntu-source-heat-engine:queens
        ks_endpoints: docker.io/kolla/ubuntu-source-heat-engine:queens
        heat_api: docker.io/kolla/ubuntu-source-heat-api:queens
        heat_cfn: docker.io/kolla/ubuntu-source-heat-api:queens
        heat_cloudwatch: docker.io/kolla/ubuntu-source-heat-api:queens
        heat_engine: docker.io/kolla/ubuntu-source-heat-engine:queens
        heat_engine_cleaner: docker.io/kolla/ubuntu-source-heat-engine:queens
      pull_policy: Always
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: heat
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/Chart/v1
metadata:
  schema: metadata/Document/v1
  name: horizon
data:
  chart_name: horizon
  release: horizon
  namespace: openstack
  timeout: 3600
  wait:
    timeout: 3600
    labels:
      release_group: taco-horizon
  install:
    no_hooks: false
  upgrade:
    no_hooks: false
  values:
    images:
      tags:
        db_init: docker.io/kolla/ubuntu-source-heat-engine:queens
        horizon_db_sync: docker.io/kolla/ubuntu-source-horizon:queens
        db_drop: docker.io/kolla/ubuntu-source-heat-engine:queens
        horizon: docker.io/kolla/ubuntu-source-horizon:queens
      pull_policy: Always
    pod:
      replicas:
        server: 1
    local_settings:
      openstack_neutron_network:
        enable_router: "True"
        enable_quotas: "True"
        enable_ipv6: "False"
        enable_distributed_router: "False"
        enable_ha_router: "True"
        enable_lb: "True"
        enable_firewall: "False"
        enable_vpn: "False"
        enable_fip_topology_check: "True"
  source:
    type: git
    location: https://github.com/sktelecom-oslab/openstack-helm
    subpath: horizon
    reference: 2.2.0
  dependencies:
    - helm-toolkit
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: infra-services
data:
  description: "Openstack Infra Services"
  sequenced: True
  chart_group:
    - ceph
    - ceph-openstack-config
    - ingress
    - etcd
    - rabbitmq
    - memcached
    - mariadb
    - libvirt
    - openvswitch
---
schema: armada/ChartGroup/v1
metadata:
  schema: metadata/Document/v1
  name: openstack-services
data:
  description: "Openstack Services"
  sequenced: False
  chart_group:
    - keystone
    - glance
    - nova
    - neutron
    - cinder
    - horizon
---
schema: armada/Manifest/v1
metadata:
  schema: metadata/Document/v1
  name: taco-manifest
data:
  release_prefix: taco
  chart_groups:
    - infra-services
    - openstack-services
